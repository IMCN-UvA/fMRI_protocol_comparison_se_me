{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrelation estimates in ROI GLMs\n",
    "\n",
    "First part of this notebook is a direct copy of 4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevenm/.conda/envs/python3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/stevenm/.conda/envs/python3/lib/python3.6/importlib/_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/stevenm/.conda/envs/python3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/stevenm/.conda/envs/python3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/stevenm/.conda/envs/python3/lib/python3.6/importlib/_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/stevenm/.conda/envs/python3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/stevenm/.conda/envs/python3/lib/python3.6/importlib/_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nideconv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=1)\n",
    "# from jupyterthemes import jtplot\n",
    "# jtplot.style('onedork')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from nilearn import masking, plotting\n",
    "import nibabel as nib\n",
    "\n",
    "# many future warnings here, these can be annoying\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from statsmodels.sandbox.stats import multicomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_data(subject_id, session, run, bids_root='./data/deriv/fmriprep', \n",
    "                   event_types=('go_trial', 'successful_stop', 'failed_stop'),\n",
    "                   get_jitters=True, drop_duplicates=True):\n",
    "    \"\"\" Loads all event data \"\"\"\n",
    "    import pandas\n",
    "\n",
    "    if session in ['optcomb', 'me', 'PAID', 'echo_1', 'echo_2', 'echo_3', 'me-den']:\n",
    "        session = 'me'\n",
    "    fn = os.path.join(bids_root, 'sub-' + subject_id, 'ses-' + session, \n",
    "                      'func', 'sub-{}_ses-{}_task-stop_run-{}_events.tsv'.format(subject_id, session, run))\n",
    "    df = pandas.read_csv(fn, sep='\\t')\n",
    "\n",
    "    # Since many subjects do not have errors and we want the design matrices to be the same(did I write this??)\n",
    "    # across subjects, we only look at correct trials\n",
    "    if event_types is not None:\n",
    "        to_return = df[np.in1d(df.trial_type, event_types)].fillna(0.1)\n",
    "    else:\n",
    "        to_return = df\n",
    "\n",
    "    if drop_duplicates:\n",
    "        # drop duplicates (double responses on a single trial)\n",
    "        to_return = to_return.drop_duplicates('onset')\n",
    "    return to_return\n",
    "\n",
    "def get_all_ts(sub, ses, run):\n",
    "    if ses == 'se':\n",
    "        dat = pd.read_csv('./deconvolution_cache/sub-{}_ses-se_run-{}_all_ts.tsv'.format(str(sub).zfill(2), run), sep='\\t')\n",
    "    elif ses in ['optcomb', 'PAID']:\n",
    "        dat = pd.read_csv('./deconvolution_cache/sub-{}_ses-me_run-{}_comb-{}_all_ts.tsv'.format(str(sub).zfill(2), run, ses), sep='\\t')\n",
    "    elif 'echo_' in ses:\n",
    "        dat = pd.read_csv('./deconvolution_cache/sub-{}_ses-me_run-{}_echo-{}_all_ts.tsv'.format(str(sub).zfill(2), run, ses[-1]), sep='\\t')\n",
    "    elif ses == 'me-den':\n",
    "        dat = pd.read_csv('./deconvolution_cache/sub-{}_ses-me-den_run-{}_comb-optcomb_all_ts.tsv'.format(str(sub).zfill(2), run), sep='\\t')\n",
    "    return dat\n",
    "\n",
    "def get_session_timeseries(ses, standardize='zscore', override_runs=None, drop_label=True):\n",
    "    dfs = []\n",
    "    for sub in range(1,19):\n",
    "        if sub == 12:\n",
    "            continue\n",
    "        elif sub == 17:\n",
    "            runs = [1,2]\n",
    "        else:\n",
    "            runs = [1,2,3]\n",
    "        sub = str(sub).zfill(2)\n",
    "        \n",
    "        if override_runs is not None:\n",
    "            runs = override_runs\n",
    "            if runs[0] == 3 and sub == 17:\n",
    "                continue\n",
    "\n",
    "        for run in runs:\n",
    "            ts = get_all_ts(sub, ses, run)\n",
    "            \n",
    "            # set on common scale across runs\n",
    "            if standardize == 'psc':\n",
    "                ts = ts.apply(lambda x: x/np.mean(x) * 100 - 100, axis=0)\n",
    "            elif standardize == 'zscore':\n",
    "                ts = ts.apply(lambda x: (x-np.mean(x)) / np.std(x), axis=0)\n",
    "                \n",
    "            ts['subject'] = sub\n",
    "            ts['run'] = run\n",
    "            if drop_label:\n",
    "                ts.drop(labels='time', axis=1, inplace=True)\n",
    "            dfs.append(ts)\n",
    "            \n",
    "    all_dat = pd.concat(dfs)\n",
    "    all_dat['session'] = ses\n",
    "    all_dat = all_dat.set_index(['subject', 'session', 'run'])\n",
    "    return all_dat\n",
    "\n",
    "def get_all_onsets(ses, event_types=('go_trial', 'failed_stop', 'successful_stop', 'response_left', 'response_right')):\n",
    "    onset_dfs = []\n",
    "    for sub in range(1,19):\n",
    "        if sub == 12:\n",
    "            continue\n",
    "        elif sub == 17:\n",
    "            runs = [1,2]\n",
    "        else:\n",
    "            runs = [1,2,3]\n",
    "        sub = str(sub).zfill(2)\n",
    "\n",
    "        for run in runs:\n",
    "            onsets_this_run = get_event_data(sub, ses, run, event_types=event_types)\n",
    "            onsets_this_run['subject'] = sub\n",
    "            onsets_this_run['session'] = ses\n",
    "            onset_dfs.append(onsets_this_run)\n",
    "\n",
    "    onsets = pd.concat(onset_dfs)\n",
    "    onsets = onsets.loc[onsets.trial_type.isin(event_types),\n",
    "                        ['subject', 'session', 'run', 'trial_type', 'onset', 'duration', 'response_time']]\n",
    "    onsets = onsets.set_index(['subject', 'session', 'run'])\n",
    "    return onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_mask_timecourses(rfGroup, data_type,\n",
    "                                 mask_name, oversample=100, plot_type='first_level',\n",
    "                                 event_types=('go_trial', 'failed_stop', 'successful_stop'),\n",
    "                                 ax=None, legend='brief', add_sigma2=False):\n",
    "    \n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots(11)\n",
    "        \n",
    "    if data_type == 't':\n",
    "        ## plot t-values\n",
    "        if rfGroup.concatenate_runs:\n",
    "            rfs = rfGroup._get_response_fitters(concatenate_runs=True)\n",
    "            t_ = rfs.apply(lambda rf: rf.get_t_value_timecourses(oversample))\n",
    "            t = pd.concat(t_.to_dict())\n",
    "            index_names = t_.index.names\n",
    "            t.index.set_names(index_names, level=range(len(index_names)), inplace=True)\n",
    "            tc_3 = t['t'].reset_index()[['subject', 'event type', 'time', mask_name]]\n",
    "            \n",
    "        else:\n",
    "            tc = rfGroup.get_t_value_timecourses(oversample=oversample, concatenate_runs=rfGroup.concatenate_runs).t\n",
    "            tc = tc[mask_name].reset_index()\n",
    "\n",
    "            # mean over runs\n",
    "            tc_2 = tc.reset_index().rename(columns={\"level_0\": 'subject', 'level_1': 'session', 'level_2': 'run'})\n",
    "            tc_3 = tc_2.pivot_table(values=mask_name, index=['subject', 'session', 'event type', 'time']).reset_index()\n",
    "        sns.lineplot(x='time', y=mask_name, hue='event type', hue_order=event_types, data=tc_3, legend=legend, ax=ax,\n",
    "                     ci=67)\n",
    "    elif data_type == 'psc':\n",
    "        ## plot psc or z-scores\n",
    "#        group_rf = rfGroup[data_type]\n",
    "        tc = rfGroup.get_subjectwise_timecourses(oversample=oversample, melt=True)\n",
    "        \n",
    "        if plot_type == 'first_level':\n",
    "            tc = tc.loc[np.in1d(tc['event type'], event_types)]\n",
    "            \n",
    "            sns.lineplot(x='time', y='value', hue='event type', hue_order=event_types, \n",
    "                         data=tc.loc[tc['roi']==mask_name], \n",
    "                         legend=legend, ax=ax, ci=67)\n",
    "\n",
    "#             sns.tsplot(data=tc.loc[tc['roi']==mask_name], \n",
    "#                        time='time', unit='subject', \n",
    "#                        condition='event type', value='value', \n",
    "#                        ax=ax, legend=legend)\n",
    "        else:\n",
    "            tc = tc.loc[tc.roi==mask_name]\n",
    "            tc = tc.loc[np.in1d(tc['event type'], ('go_trial', 'failed_stop', 'successful_stop'))]\n",
    "            tc = tc.pivot_table(values='value', index=['subject', 'time', 'covariate'], columns='event type').reset_index()\n",
    "            tc['failed_stop - go_trial'] = tc['failed_stop'] - tc['go_trial']\n",
    "            tc['failed_stop - successful_stop'] = tc['failed_stop'] - tc['successful_stop']\n",
    "            tc['successful_stop - go_trial'] = tc['successful_stop'] - tc['go_trial']\n",
    "            tc = tc.melt(id_vars=['subject', 'time'], value_vars=['failed_stop - go_trial', 'failed_stop - successful_stop', 'successful_stop - go_trial'])\n",
    "            sns.lineplot(x='time', y='value', hue='event type', data=tc, legend=legend, ax=ax)\n",
    "    elif data_type == 't-level2':\n",
    "        tc = rfGroup.get_subjectwise_timecourses(oversample=oversample, melt=True)\n",
    "        tc = tc.loc[np.in1d(tc['event type'], event_types)]\n",
    "        \n",
    "        # t-test\n",
    "        t_vals = tc.groupby(['event type', 'roi', 'time'])['value'].apply(lambda x: stats.ttest_1samp(x, 0)[0]).reset_index()\n",
    "        sns.lineplot(data=t_vals.loc[t_vals['roi']==mask_name], \n",
    "                     x='time', hue='event type', y='value', \n",
    "                     ax=ax, legend=legend)\n",
    "    elif data_type == 'snr-level2':\n",
    "        tc = rfGroup.get_subjectwise_timecourses(oversample=oversample, melt=True)\n",
    "        tc = tc.loc[np.in1d(tc['event type'], event_types)]\n",
    "        \n",
    "        # t-test\n",
    "        vals = tc.groupby(['event type', 'roi', 'time'])['value'].apply(lambda x: x.mean()/x.std()).reset_index()\n",
    "        sns.lineplot(data=vals.loc[vals['roi']==mask_name], \n",
    "                     x='time', hue='event type', y='value', \n",
    "                     ax=ax, legend=legend, hue_order=event_types)\n",
    "    \n",
    "    if add_sigma2:\n",
    "        sigma2 = rfGroup._get_response_fitters().apply(lambda x: x.sigma2)[mask_name]\n",
    "        t = ax.text(.95, .95, '$\\sigma^2 = {}\\pm{}$'.format(np.mean(sigma2).round(2), np.std(sigma2).round(2)),\n",
    "                     horizontalalignment='right',\n",
    "                     verticalalignment='top',\n",
    "                     transform = ax.transAxes)\n",
    "        t.set_bbox(dict(facecolor='white', alpha=1.0, edgecolor='grey'))\n",
    "        \n",
    "    ax.axhline(y=0, c='k', ls='--')\n",
    "    ax.axvline(x=0, c='k', ls='--')\n",
    "    \n",
    "from matplotlib import gridspec\n",
    "    \n",
    "def make_plot(plot_dict, data_type, columns, masks, oversample=100, plot_type='first_level', \n",
    "              y_label='% signal change', event_types=('failed_stop', 'go_trial', 'successful_stop'),\n",
    "              column_titles=None, add_sigma2=False, sharey='row'):\n",
    "    \n",
    "    if len(columns) == 5 and not sharey == 'row':\n",
    "        width_ratios = [1, 0.1, 1, 0.3, 1, .1, 1, .1, 1]\n",
    "    else:\n",
    "        width_ratios = [1, .15] * (len(columns)-1) + [1]\n",
    "    gridspec_kws = dict(hspace=0.1, wspace=0.0, \n",
    "                        width_ratios=width_ratios)\n",
    "    f, axes = plt.subplots(len(masks), len(width_ratios), gridspec_kw=gridspec_kws)\n",
    "\n",
    "    columns_to_populate = np.arange(len(columns), dtype=int)*2\n",
    "    empty_columns = [x for x in np.arange(len(width_ratios)) if not x in columns_to_populate]\n",
    "    for col in empty_columns:\n",
    "        for row in range(len(masks)):\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "#     f, axes = plt.subplots(len(masks), len(columns))\n",
    "    if len(axes.shape) == 1:\n",
    "        axes = axes[np.newaxis,:]\n",
    "\n",
    "#     ax = f.add_subplot(111, frameon=False)\n",
    "#     # hide tick and tick label of the big axes\n",
    "# #     ax.axis('off')\n",
    "#     ax.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "#     ax.spines[\"left\"].set_visible(False)\n",
    "#     ax.spines[\"top\"].set_visible(False)\n",
    "#     ax.spines[\"right\"].set_visible(False)\n",
    "#     ax.spines[\"bottom\"].set_visible(False)\n",
    "#     ax.set_ylabel(y_label)\n",
    "#     # ax.set_xlabel('Time from trial onset (s)')\n",
    "#     ax.yaxis.labelpad = 30\n",
    "#    fig.text(0.5, 0.04, 'common X', ha='center')\n",
    "    f.text(0.05, 0.5, y_label, va='center', rotation='vertical')#, labelpad=30)\n",
    "\n",
    "    # for titles: https://stackoverflow.com/questions/40936729/matplotlib-title-spanning-two-or-any-number-of-subplot-columns\n",
    "    ext = []\n",
    "    # loop over the columns (j) and rows(i) to populate subplots\n",
    "    for j in range(len(axes[0,:])):\n",
    "        # save the axes bounding boxes for later use\n",
    "        ext.append([axes[0,j].get_window_extent().x0, axes[0,j].get_window_extent().width])\n",
    "\n",
    "    # make nice\n",
    "    inv = f.transFigure.inverted()\n",
    "    width_left = ext[0][0]+(ext[columns_to_populate.max()][0]+ext[columns_to_populate.max()][1]-ext[0][0])/2.\n",
    "    left_center = inv.transform( (width_left, 1) )\n",
    "\n",
    "    # set column spanning title \n",
    "    # the first two arguments to figtext are x and y coordinates in the figure system (0 to 1)\n",
    "    plt.figtext(left_center[0], .1, \"Time from trial onset (s)\", va=\"center\", ha=\"center\")\n",
    "\n",
    "    # plot\n",
    "    for row, mask in enumerate(masks):\n",
    "        print(mask, end='')\n",
    "        for col_n, session_name in zip(columns_to_populate, columns):\n",
    "    \n",
    "            if isinstance(oversample, list):\n",
    "                oversample_ = oversample[col_n]\n",
    "            else:\n",
    "                oversample_ = oversample\n",
    "            print('.', end='')\n",
    "            plot_single_mask_timecourses(plot_dict[session_name],\n",
    "                                         data_type=data_type,\n",
    "                                         mask_name=mask, \n",
    "                                         ax=axes[row,col_n], \n",
    "                                         plot_type=plot_type,\n",
    "                                         oversample=oversample_,\n",
    "                                         event_types=event_types,\n",
    "                                         add_sigma2=add_sigma2)\n",
    "            if col_n > 0:\n",
    "                axes[row, col_n].set_ylabel('')\n",
    "            # remove xlabels except bottom row\n",
    "#             if row < (len(masks)-1):\n",
    "            axes[row, col_n].set_xlabel('')\n",
    "        axes[row,0].set_ylabel(mask)\n",
    "\n",
    "        if sharey == 'row':\n",
    "            # get maximum ylims, set\n",
    "            current_ranges = np.array([np.array(x.get_ylim()) for x in axes[row,:]])\n",
    "            for ax in axes[row,:]:\n",
    "                ax.set_ylim(current_ranges.min(0)[0], current_ranges.max(0)[1])\n",
    "        else:\n",
    "            for sharey_columns in sharey:\n",
    "                sharey_columns = [x*2 for x in sharey_columns]\n",
    "                # get maximum ylims, set\n",
    "                current_ranges = np.array([np.array(x.get_ylim()) for x in axes[row, sharey_columns]])\n",
    "                for ax in axes[row, sharey_columns]:\n",
    "                    ax.set_ylim(current_ranges.min(0)[0], current_ranges.max(0)[1])\n",
    "\n",
    "    # Add column titles\n",
    "    if column_titles is None:\n",
    "        column_titles = columns\n",
    "    \n",
    "    for col_n, col_name in zip(columns_to_populate, column_titles):\n",
    "        axes[0, col_n].set_title(col_name.replace('_', ' '))\n",
    "\n",
    "    if plot_type == 'contrasts':\n",
    "        labels = [\"Failed stop - go\", \"Failed stop - successful stop\", \"Successful stop - go\"]\n",
    "    else:\n",
    "        labels = ['Failed stop', 'Go', 'Successful stop']\n",
    "        \n",
    "    # remove legends except first\n",
    "    for row_n in range(len(masks)):\n",
    "        for col_n in columns_to_populate:\n",
    "            if col_n == 4 and row_n == 0:\n",
    "                axes[row_n, col_n].legend(handles=axes[row_n, col_n].lines, \n",
    "                                          labels=labels,\n",
    "                                          loc=9, ncol=3, bbox_to_anchor=(.5, 1.4))\n",
    "#                 axes[row_n, col_n].legend_.set_bbox_to_anchor((1, 1), transform=axes[row_n, col_n].transAxes)\n",
    "            else:\n",
    "                axes[row_n, col_n].legend_.remove()\n",
    "    \n",
    "    # determine where to place/remove ticks on y-axis\n",
    "    if sharey == 'row':\n",
    "        keep_y_ticks = [0]\n",
    "    else:\n",
    "        keep_y_ticks = [x[0]*2 for x in sharey]\n",
    "        \n",
    "    # hide ticks/values on axes\n",
    "    for col in np.arange(1, columns_to_populate.max()+1):\n",
    "        if col in keep_y_ticks:\n",
    "            labelleft = 'On'\n",
    "        else:\n",
    "            labelleft = 'Off'\n",
    "\n",
    "        for row in range(0, len(masks)-1):\n",
    "            axes[row, col].tick_params(top='off', right='off', labelbottom='Off', labelleft=labelleft)\n",
    "\n",
    "        # for bottom row, keep bottom labels\n",
    "        axes[len(masks)-1,col].tick_params(top='off', right='off', labelleft=labelleft)\n",
    "\n",
    "    for row in range(0, len(masks)-1):\n",
    "#        axes[row, -1].tick_params(top='off', right='off', labelbottom='Off')\n",
    "        for col in keep_y_ticks:\n",
    "            axes[row, col].tick_params(top='off', right='off', labelbottom='Off')\n",
    "    \n",
    "\n",
    "    f.set_size_inches(len(columns)*4, len(masks)*3)\n",
    "    return f, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses_ts = {'se': {'psc': get_session_timeseries('se', standardize='psc'),\n",
    "                 'zscore': get_session_timeseries('se', standardize='zscore')},\n",
    "          'optcomb': {'psc': get_session_timeseries('optcomb', standardize='psc'),\n",
    "                      'zscore': get_session_timeseries('optcomb', standardize='zscore')},\n",
    "          'echo_1': {'psc': get_session_timeseries('echo_1', standardize='psc'),\n",
    "                     'zscore': get_session_timeseries('echo_1', standardize='zscore')},\n",
    "          'echo_2': {'psc': get_session_timeseries('echo_2', standardize='psc'),\n",
    "                     'zscore': get_session_timeseries('echo_2', standardize='zscore')},\n",
    "          'echo_3': {'psc': get_session_timeseries('echo_3', standardize='psc'),\n",
    "                     'zscore': get_session_timeseries('echo_3', standardize='zscore')}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit canonical HRF with temporal derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get motion parameters as confounds\n",
    "def get_confounds_timeseries(ses, standardize='psc', override_runs=None):\n",
    "    dfs = []\n",
    "    for sub in range(1,19):\n",
    "        if sub == 12:\n",
    "            continue\n",
    "        elif sub == 17:\n",
    "            runs = [1,2]\n",
    "        else:\n",
    "            runs = [1,2,3]\n",
    "        sub_str = str(sub).zfill(2)\n",
    "        \n",
    "        if override_runs is not None:\n",
    "            runs = override_runs\n",
    "            if runs[0] == 3 and sub == 17:\n",
    "                continue\n",
    "\n",
    "        for run in runs:\n",
    "            if ses == 'se':\n",
    "                ts = pd.read_csv('./data/deriv/fmriprep/sub-{}/ses-se/func/sub-{}_ses-se_task-stop_run-{}_desc-confounds_regressors.tsv'.format(sub_str, sub_str, run), sep='\\t')\n",
    "            elif ses == 'optcomb':\n",
    "                ts = pd.read_csv('./data/deriv/fmriprep/sub-{}/ses-me/func/sub-{}_ses-me_task-stop_run-{}_echo-1_desc-confounds_regressors.tsv'.format(sub_str, sub_str, run), sep='\\t')\n",
    "            elif 'echo' in ses:\n",
    "                echo_n = ses[-1]\n",
    "                ts = pd.read_csv('./data/deriv/fmriprep/sub-{}/ses-me/func/sub-{}_ses-me_task-stop_run-{}_echo-{}_desc-confounds_regressors.tsv'.format(sub_str, sub_str, run, echo_n), sep='\\t')\n",
    "            \n",
    "            ts['subject'] = sub_str\n",
    "            ts['run'] = run\n",
    "            if 'time' in ts.columns:\n",
    "                ts.drop(labels='time', axis=1, inplace=True)\n",
    "            dfs.append(ts)\n",
    "            \n",
    "    all_dat = pd.concat(dfs)\n",
    "    all_dat['session'] = ses\n",
    "    all_dat = all_dat.set_index(['subject', 'session', 'run'])\n",
    "    return all_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: se...psc..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No event type was given, automatically entering the following event types: Index(['go_trial', 'successful_stop', 'failed_stop'], dtype='object', name='trial_type')\n",
      "/home/stevenm/.conda/envs/python3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2843: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  raw_cell, store_history, silent, shell_futures)\n",
      "/home/stevenm/.conda/envs/python3/lib/python3.6/site-packages/pandas/core/indexing.py:1494: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: optcomb...psc..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No event type was given, automatically entering the following event types: Index(['go_trial', 'failed_stop', 'successful_stop'], dtype='object', name='trial_type')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: echo_1...psc..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No event type was given, automatically entering the following event types: Index(['go_trial', 'failed_stop', 'successful_stop'], dtype='object', name='trial_type')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: echo_2...psc..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No event type was given, automatically entering the following event types: Index(['go_trial', 'failed_stop', 'successful_stop'], dtype='object', name='trial_type')\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "#masks = ['rSTN', 'lSTN', 'rSTR', 'lSTR', 'lGPe', 'rGPe', 'lPreSMA', 'rPreSMA', 'rM1', 'rIFG']\n",
    "\n",
    "\n",
    "rfGroupsCwD = {'psc': {}} #, 'zscore': {}}\n",
    "for ses in ['se', 'optcomb', 'echo_1', 'echo_2', 'echo_3']:\n",
    "    print('Data: {}...'.format(ses), end='')\n",
    "    onsets = get_all_onsets(ses=ses, event_types=('go_trial', 'successful_stop', 'failed_stop'))\n",
    "    # NB: slice time correction references to TR/2, not to 0; shift onsets to align timesereis and events\n",
    "    onsets['onset'] -= 1.5\n",
    "    \n",
    "    # exclude trials with RT > 1\n",
    "    onsets['rt'] = onsets['response_time'] - onsets['onset'] - 1.5\n",
    "    onsets = onsets.loc[~((onsets['rt'] > 1) & (onsets['trial_type'] != 'successful_stop'))]\n",
    "    del onsets['duration']\n",
    "    del onsets['response_time']\n",
    "    del onsets['rt']\n",
    "\n",
    "    for standardization in rfGroupsCwD.keys():\n",
    "        print('{}...'.format(standardization), end='')\n",
    "        # get relevant data\n",
    "        ts = ses_ts[ses][standardization]\n",
    "        confounds = get_confounds_timeseries(ses)[['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', 'framewise_displacement']]\n",
    "        confounds = confounds.reset_index()\n",
    "        \n",
    "        # deconvolve\n",
    "        rfGroup = nideconv.GroupResponseFitter(ts, onsets, 1/3., confounds=confounds,\n",
    "                                               oversample_design_matrix=20)\n",
    "        rfGroup.add_event(interval=[0,18], basis_set='canonical_hrf_with_time_derivative')\n",
    "        rfGroup.fit(store_residuals=True, type='ar(1)')\n",
    "\n",
    "        rfGroupsCwD[standardization][ses] = rfGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['se', 'optcomb', 'echo_1', 'echo_2', 'echo_3']\n",
    "plot_masks = ['rM1', 'rPreSMA', 'rIFG', 'rSTR', 'rGPe', 'rGPi', 'rSTN']\n",
    "\n",
    "f, ax = make_plot(plot_dict=rfGroupsCwD['psc'],\n",
    "                  y_label='% signal change',\n",
    "                  columns=columns, \n",
    "                  masks=plot_masks, \n",
    "                  data_type='psc', \n",
    "                  oversample=1,\n",
    "                  plot_type='first_level',\n",
    "                  add_sigma2=True,\n",
    "                  sharey=[(0, 1), (2, 3, 4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of first-level $t$-values between protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "\n",
    "def get_t_vals(rfGroup, data_type='se', contrast_type='first_level'):\n",
    "    \"\"\" Extract t-values (PEs, PE variance) from rfGroup-object \"\"\"\n",
    "    contrasts = {}\n",
    "    \n",
    "    # Get regressor names, construct contrast vector\n",
    "    X = rfGroup._get_response_fitters()[0].X\n",
    "    regressor_names = np.array(['_'.join(col).strip() for col in X.columns.values])\n",
    "    successful_stop_vs_baseline = (regressor_names == 'successful_stop_intercept_canonical_hrf').astype(int)\n",
    "    failed_stop_vs_baseline = (regressor_names == 'failed_stop_intercept_canonical_hrf').astype(int)\n",
    "    go_trial_vs_baseline = (regressor_names == 'go_trial_intercept_canonical_hrf').astype(int)\n",
    "    \n",
    "    if contrast_type == 'first_level':\n",
    "        contrasts['Successful stop'] = successful_stop_vs_baseline\n",
    "        contrasts['Failed stop'] = failed_stop_vs_baseline\n",
    "        contrasts['Go'] = go_trial_vs_baseline\n",
    "    else:\n",
    "        contrasts['Failed stop - go'] = failed_stop_vs_baseline - go_trial_vs_baseline\n",
    "        contrasts['Failed stop - successful stop'] = failed_stop_vs_baseline - successful_stop_vs_baseline\n",
    "        contrasts['Successful stop - go'] = successful_stop_vs_baseline - go_trial_vs_baseline\n",
    "    from pprint import pprint\n",
    "    pprint(contrasts)\n",
    "\n",
    "#         if data_type == 'se':\n",
    "#             contrasts['Successful stop'] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "#             contrasts['Failed stop'] =     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "#             contrasts['Go'] =              [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "#         else:\n",
    "#             contrasts['Successful stop'] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "#             contrasts['Failed stop'] =     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "#             contrasts['Go'] =              [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "#     else:\n",
    "#         if data_type == 'se':\n",
    "#             contrasts['Failed stop - go'] =              [0, 0, 0, 0, 0, 0, 0, 0, -1, 0,  0, 0, 1, 0]\n",
    "#             contrasts['Failed stop - successful stop'] = [0, 0, 0, 0, 0, 0, 0, 0,  0, 0, -1, 0, 1, 0]\n",
    "#             contrasts['Successful stop - go'] =          [0, 0, 0, 0, 0, 0, 0, 0, -1, 0,  1, 0, 0, 0]\n",
    "#         else:\n",
    "#             contrasts['Failed stop - go'] =              [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0,  0, 0]\n",
    "#             contrasts['Failed stop - successful stop'] = [0, 0, 0, 0, 0, 0, 0, 0,  0, 0, 1, 0, -1, 0]\n",
    "#             contrasts['Successful stop - go'] =          [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0,  1, 0]\n",
    "\n",
    "    pes = []\n",
    "    var_pes = []\n",
    "    ts = []\n",
    "    for c_name, c_vector in contrasts.items():\n",
    "        pe = rfGroup._get_response_fitters().apply(lambda x: estimate_contrast(x, c_vector, return_type='cope'))\n",
    "        var_pe = rfGroup._get_response_fitters().apply(lambda x: estimate_contrast(x, c_vector, return_type='varcope'))\n",
    "        t = pe / np.sqrt(var_pe)\n",
    "        \n",
    "        pe['Event'] = c_name\n",
    "        var_pe['Event'] = c_name\n",
    "        t['Event'] = c_name\n",
    "        \n",
    "        pes.append(pe)\n",
    "        var_pes.append(var_pe)\n",
    "        ts.append(t)\n",
    "\n",
    "    return pd.concat(pes), pd.concat(var_pes), pd.concat(ts)\n",
    "    \n",
    "def estimate_contrast(response_fitter, C, return_type='cope'):\n",
    "    X = response_fitter.X\n",
    "    ys = response_fitter.input_signal\n",
    "    all_copes = []\n",
    "    all_varcopes = []\n",
    "    if ys.shape[1] > 1:\n",
    "        for roi_idx, roi in enumerate(ys.columns):\n",
    "#             print(roi)\n",
    "            this_gls = response_fitter.gls_models[roi_idx]\n",
    "            this_gls_res = response_fitter.gls_results[roi_idx]\n",
    "            this_sigma2 = response_fitter\n",
    "#             this_y = ys[roi].copy()\n",
    "#             this_X = X.copy()\n",
    "#             ols_resid = sm.OLS(this_y, this_X).fit().resid.values\n",
    "#             resid_fit = sm.OLS(ols_resid[1:], sm.add_constant(ols_resid[:-1])).fit()\n",
    "#             rho = resid_fit.params[1]  # this is our rho\n",
    "\n",
    "#             # make V\n",
    "#             order = sp.linalg.toeplitz(np.arange(len(this_y)))\n",
    "#             V = rho ** order\n",
    "\n",
    "#             gls = sm.GLS(this_y, this_X, sigma=V)\n",
    "#             gls_res = gls.fit()\n",
    "            \n",
    "            # prewhitened design variance\n",
    "            wX = this_gls.wexog\n",
    "            XtXinv = np.linalg.pinv(np.dot(wX.T, wX))\n",
    "            \n",
    "            # error variance\n",
    "            sigma2 = response_fitter.sigma2.values[roi_idx]\n",
    "            \n",
    "            # contrast\n",
    "            cope = np.dot(C, this_gls_res.params)\n",
    "            varcope = sigma2*np.dot(np.dot(C, XtXinv), C)\n",
    "            \n",
    "\n",
    "            all_copes.append(cope)\n",
    "            all_varcopes.append(varcope)\n",
    "            \n",
    "            \n",
    "            ## manually below\n",
    "#             W = np.linalg.cholesky(np.linalg.pinv(V))\n",
    "\n",
    "#             # whiten\n",
    "#             this_X_w = np.dot(W, this_X)\n",
    "#             this_y_w = np.dot(W, this_y)\n",
    "\n",
    "#             # pes\n",
    "#             XtXinv = np.linalg.pinv(np.dot(this_X_w.T, this_X_w))\n",
    "#             XY = np.dot(this_X_w.T, this_y_w)\n",
    "#             betas = np.dot(XtXinv, XY)\n",
    "\n",
    "#             # copes\n",
    "#             copes = np.dot(C, betas)\n",
    "\n",
    "#             # var; get sigma^2 of the model\n",
    "#             pred = this_X_w.dot(betas)\n",
    "#             resid = this_y_w - pred\n",
    "\n",
    "            # variance\n",
    "#            XtXinv = np.linalg.pinv(np.dot(wX.T, wX))\n",
    "#            e = this_y - gls_res.predict()\n",
    "#            sigma2 = (np.dot(e.T, e)).sum()/(this_y.shape[0]-(this_X.shape[1]))\n",
    "            #(np.dot(e.T, e)).sum()/(this_y.shape[0]-(this_X.shape[1]))  # sum of squared errors / (T - p)\n",
    "#            varcope = sigma2*np.dot(np.dot(C, XtXinv), C)\n",
    "\n",
    "#             varcope = sigma2*np.dot(np.dot(C, XtXinv), C)\n",
    "\n",
    "#             all_copes.append(copes)\n",
    "#             all_varcopes.append(varcope)\n",
    "    if return_type == 'cope':\n",
    "        return pd.Series(all_copes, index=ys.columns)\n",
    "    elif return_type == 'varcope':\n",
    "        return pd.Series(all_varcopes, index=ys.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first-level t-values for the single echo, optimally combined multi echo data, and per echo\n",
    "pe_se, var_se, t_se = get_t_vals(rfGroupsCwD['psc']['se'], data_type='se')\n",
    "pe_oc, var_oc, t_oc = get_t_vals(rfGroupsCwD['psc']['optcomb'], data_type='oc')\n",
    "pe_e1, var_e1, t_e1 = get_t_vals(rfGroupsCwD['psc']['echo_1'], data_type='e1')\n",
    "pe_e2, var_e2, t_e2 = get_t_vals(rfGroupsCwD['psc']['echo_2'], data_type='e2')\n",
    "pe_e3, var_e3, t_e3 = get_t_vals(rfGroupsCwD['psc']['echo_3'], data_type='e3')\n",
    "\n",
    "# Add name to data frames\n",
    "t_se['Data'] = 'Single echo'\n",
    "t_oc['Data'] = 'Multi echo (OC)'\n",
    "t_e1['Data'] = 'Echo 1'\n",
    "t_e2['Data'] = 'Echo 2'\n",
    "t_e3['Data'] = 'Echo 3'\n",
    "\n",
    "pe_se['Data'] = 'Single echo'\n",
    "pe_oc['Data'] = 'Multi echo (OC)'\n",
    "pe_e1['Data'] = 'Echo 1'\n",
    "pe_e2['Data'] = 'Echo 2'\n",
    "pe_e3['Data'] = 'Echo 3'\n",
    "\n",
    "# Add contrasts\n",
    "pe_se_contrast, var_se_contrast, t_se_contrast = get_t_vals(rfGroupsCwD['psc']['se'], data_type='se', contrast_type='contrast')\n",
    "pe_oc_contrast, var_oc_contrast, t_oc_contrast = get_t_vals(rfGroupsCwD['psc']['optcomb'], data_type='oc', contrast_type='contrast')\n",
    "pe_e1_contrast, var_e1_contrast, t_e1_contrast = get_t_vals(rfGroupsCwD['psc']['echo_1'], data_type='e1', contrast_type='contrast')\n",
    "pe_e2_contrast, var_e2_contrast, t_e2_contrast = get_t_vals(rfGroupsCwD['psc']['echo_2'], data_type='e2', contrast_type='contrast')\n",
    "pe_e3_contrast, var_e3_contrast, t_e3_contrast = get_t_vals(rfGroupsCwD['psc']['echo_3'], data_type='e3', contrast_type='contrast')\n",
    "\n",
    "t_se_contrast['Data'] = 'Single echo'\n",
    "t_oc_contrast['Data'] = 'Multi echo (OC)'\n",
    "t_e1_contrast['Data'] = 'Echo 1'\n",
    "t_e2_contrast['Data'] = 'Echo 2'\n",
    "t_e3_contrast['Data'] = 'Echo 3'\n",
    "\n",
    "pe_se_contrast['Data'] = 'Single echo'\n",
    "pe_oc_contrast['Data'] = 'Multi echo (OC)'\n",
    "pe_e1_contrast['Data'] = 'Echo 1'\n",
    "pe_e2_contrast['Data'] = 'Echo 2'\n",
    "pe_e3_contrast['Data'] = 'Echo 3'\n",
    "\n",
    "# Concatenate everything into pd.DataFrames\n",
    "pes = pd.concat([pe_se, pe_oc, pe_e1, pe_e2, pe_e3, pe_se_contrast, pe_oc_contrast, pe_e1_contrast, pe_e2_contrast, pe_e3_contrast])\n",
    "t_vals = pd.concat([t_se, t_oc, t_e1, t_e2, t_e3, t_se_contrast, t_oc_contrast, t_e1_contrast, t_e2_contrast, t_e3_contrast])\n",
    "\n",
    "# Make them \"long\" instead of \"wide\"\n",
    "pes_long = pes.reset_index().melt(id_vars=['subject', 'Data', 'Event'], var_name='ROI', value_name='pe')\n",
    "t_vals_long = t_vals.reset_index().melt(id_vars=['subject', 'Data', 'Event'], var_name='ROI', value_name='t')\n",
    "\n",
    "pes_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf(x, length=10):\n",
    "    return np.array([1]+[np.corrcoef(x[:-i], x[i:])[0,1]  \\\n",
    "        for i in range(1, length)])\n",
    "\n",
    "def residuals_acf(x, roi_idx=0, whitened_residuals=True):\n",
    "    \n",
    "    use_residuals = x._residuals.iloc[:,roi_idx].values\n",
    "    if whitened_residuals:\n",
    "        use_residuals = x.gls_models[roi_idx].cholsigmainv.dot(use_residuals)\n",
    "\n",
    "    return pd.Series(acf(use_residuals), index=np.arange(10))\n",
    "\n",
    "# get roi names\n",
    "tmp = rfGroupsCwD['psc']['se']._get_response_fitters()[0]\n",
    "roi_names = [x for x in tmp.input_signal.columns]\n",
    "\n",
    "## plot\n",
    "lags = np.arange(10)\n",
    "f, axes = plt.subplots(nrows=4, ncols=4)\n",
    "ses_lab_dict = {'se': 'Single echo', 'optcomb': 'Multi echo (OC)', \n",
    "                'echo_1': 'Echo 1', 'echo_2': 'Echo 2', 'echo_3': 'Echo 3'}\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.axhline(color='black')\n",
    "    \n",
    "    roi_name = roi_names[i]\n",
    "    for ii, (ses, lab) in enumerate(ses_lab_dict.items()):\n",
    "        resids_rho = rfGroupsCwD['psc'][ses]._get_response_fitters().apply(residuals_acf, roi_idx=i)\n",
    "        mns = resids_rho.mean(axis=0)\n",
    "        stds = resids_rho.std(axis=0)\n",
    "        ax.errorbar(x=lags+.075*ii, y=mns, yerr=stds, capsize=3, elinewidth=1, label=lab)\n",
    "    ax.set_title(roi_name)\n",
    "    ax.set_ylim([-.3, .4])\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "f.text(0.5, 0, 'Lag (volumes)', ha='center', va='center')\n",
    "f.text(0.00, 0.5, 'Autocorrelation whitened residuals', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "f.set_size_inches(16, 16)\n",
    "f.tight_layout()\n",
    "f.savefig('./figures/autocorrelation_whitened_residuals.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the remaining autocorrelation pattern does not seem to depend on the protocol / data type, this suggests (to me) that it's physiological autocorrelation. This also explains why there's minor differences in the remaining autocorrelation in lSTN (no signal / no physiological signal left in the later echos, so also no autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about rhos per ROI?\n",
    "df = []\n",
    "for ses, label in ses_lab_dict.items():\n",
    "    rhos = rfGroupsCwD['psc'][ses]._get_response_fitters().apply(lambda x: pd.Series(x.rhos[:,0], index=roi_names))\n",
    "    rhos = rhos.reset_index().melt(id_vars='subject', value_name='rho', var_name='ROI')\n",
    "    rhos['Data'] = label\n",
    "    df.append(rhos)\n",
    "rhos = pd.concat(df)\n",
    "\n",
    "sns.barplot(x='ROI', y='rho', hue='Data', data=rhos, order=['rM1', 'rPreSMA', 'rIFG', 'rSTR', 'rGPe', 'rGPi', 'rSTN'])\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "\n",
    "# so yes there's some rho differences, again mainly in subcortical areas\n",
    "# Speculatively, this is a consequence of the low tSNR of the later echoes in subcortical areas;\n",
    "# most of the 'signal' is actually noise, which is pretty much white at the last echo.\n",
    "\n",
    "# the residual autocorrelation appears to be related to \"signal\" (something physiological?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlations between residuals of different echoes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 3)\n",
    "axes = axes.ravel()\n",
    "## variance-covariance matrix of residuals of echos\n",
    "for i, roi in enumerate(['rM1', 'rPreSMA', 'rIFG', 'rSTR', 'rGPe', 'rGPi', 'rSTN']):\n",
    "    roi_idx = np.where(np.array(roi_names) == roi)[0][0]\n",
    "    whitened_residuals1 = rfGroupsCwD['psc']['echo_1']._get_response_fitters().apply(lambda rf: rf.gls_models[roi_idx].cholsigmainv.dot(rf._residuals.iloc[:,roi_idx]))\n",
    "    whitened_residuals2 = rfGroupsCwD['psc']['echo_2']._get_response_fitters().apply(lambda rf: rf.gls_models[roi_idx].cholsigmainv.dot(rf._residuals.iloc[:,roi_idx]))\n",
    "    whitened_residuals3 = rfGroupsCwD['psc']['echo_3']._get_response_fitters().apply(lambda rf: rf.gls_models[roi_idx].cholsigmainv.dot(rf._residuals.iloc[:,roi_idx]))\n",
    "\n",
    "    residual_df = pd.concat([whitened_residuals1, whitened_residuals2, whitened_residuals3], axis=1)\n",
    "    corrmat = residual_df.apply(lambda x: np.corrcoef(np.vstack([x[0], x[1], x[2]])), axis=1).mean(axis=0)\n",
    "    corrmat = pd.DataFrame(corrmat, index=['Echo 1', 'Echo 2', 'Echo 3'], columns = ['Echo 1', 'Echo 2', 'Echo 3'])\n",
    "    sns.heatmap(corrmat, ax=axes[i], vmax=1, center=0, square=True, linewidths=.5, \n",
    "                cbar_kws={\"shrink\": .5}, annot=True, cbar=False)\n",
    "    if i < 4:\n",
    "        axes[i].tick_params(axis='x',          # changes apply to the x-axis\n",
    "                            which='both',      # both major and minor ticks are affected\n",
    "                            bottom=False,      # ticks along the bottom edge are off\n",
    "                            top=False,         # ticks along the top edge are off\n",
    "                            labelbottom=False) # labels along the \n",
    "    if i not in [0, 3, 6]:\n",
    "        axes[i].tick_params(axis='y',          # changes apply to the x-axis\n",
    "                            which='both',      # both major and minor ticks are affected\n",
    "                            bottom=False,      # ticks along the bottom edge are off\n",
    "                            top=False,\n",
    "                            left=False,\n",
    "                            labelleft=False) # labels along the\n",
    "    axes[i].set_title(roi)\n",
    "    \n",
    "axes[-2].grid('Off')\n",
    "axes[-1].grid('Off')\n",
    "axes[-1].axis('off')\n",
    "axes[-2].axis('off')\n",
    "f.set_size_inches(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example plot of residuals for first subject\n",
    "plt.plot(residual_df.iloc[0][0][:20])\n",
    "plt.plot(residual_df.iloc[0][1][:20])\n",
    "plt.plot(residual_df.iloc[0][2][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
